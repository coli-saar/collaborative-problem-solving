from random import randrange, choices, randint
import re
import os
import ast
from openai import OpenAI
from supplementary.config import *
from AGENTS.prompts_baseline import *
from datetime import datetime
import json
from itertools import combinations


class GAME:
    def __init__(self, N:int, BOARD:dict, SEED:int, LOG_DIAL:str, LOG_EVAL:str, LOG_GSM:str, BOARD_ID:int, ID:int):
        ''' The GAME-governing class
            
            :param N: int; number of nodes
            :param BOARD: dict; dict contianing the boards for both agents, indexed under roles (BOT, USER).
            :param SEED: int; batch marker; same for both agents
            :param LOG_DIAL: str; path to dialogue log file
            :param LOG_EVAL: str; path to eval log file
            :param LOG_GSM: str;  path to GroundStateManager (GSM) log file
            :param BOARD_ID: int; the ID of the board (used for logging)
            :param ID: int; the ID of the game within the batch (used for logging)
            '''

        self.run_start = datetime.now()
        self.id = ID
        self.prompt_type = ""

        self.N = N # number of nodes
        self.other_user = {"BOT":"USER", "USER":"BOT"} # structure for efficient indexing of the "other" user, when given the current user as key

        self.rooms = {"BOT": ["L", "E", "K", "B"], "USER": ["L", "E", "K", "B"]} # list of available rooms (default: 4-nodes)
        
        # add other rooms depending on the number of nodes
        if self.N >= 5:
            self.rooms["BOT"].append("C")
            self.rooms["USER"].append("C")
        if self.N == 6:
            self.rooms["BOT"].append("A")
            self.rooms["USER"].append("A")

        # Logging files
        self.logging_dialogues = LOG_DIAL # logging file containing all dialogues
        self.logging_GSM = LOG_GSM # logging file containing all ground state manager outputs
        self.logging_eval = LOG_EVAL # logging file containing all evaluation info

         # Graph contents - read from json outside of class
        self.boards = {"BOT":BOARD["BOT"], "USER":BOARD["USER"]}  # the boards given to the players
        self.solutions = {"BOT":[], "USER":[], "OPTIMAL":BOARD["OPTIMAL"]} # storing the produced solutions for evaluation; OPTIMAL is the path that returns the most joint coins
        self.BOARD_ID = BOARD_ID

        # Flags
        self.playing = True # True = the game is ongoing

        # DATA STRUCTURES
        self.actions_history = {"BOT":[], "USER":[]} #list of  not used in this version
        self.agreements_history = {"BOT":[], "USER":[]} # might not be used, TBD
        self.end = {"BOT":False, "USER":False} # dict for storing whether an "end" actioin was generated by each   
        
        # Agent params
        self.seed = SEED
        self.model = "gpt-4o-2024-08-06"
        self.bot_params = self.get_params("BOT")
        self.user_params = self.get_params("USER")

        # Players
        self.bot = BotInstance(BASE_PROMPT=GHOST_prompt(self.bot_params, self.N), MODEL=self.model, SEED=self.seed, MAXTOKENS=1200, BOARD=BOARD["BOT"]) # bot agent
        self.user_proxy = BotInstance(BASE_PROMPT=USER_LIGHT_prompt(self.user_params, self.N), MODEL=self.model, SEED=self.seed, MAXTOKENS=1200, BOARD=BOARD["USER"]) # user agent
        
        # Eval flags:
            # identical - are the two agents' solutions identical?
            # adequate - is the solution of proper length? (may not be optimal)
            # optimal - is the given solution optimal?
            # empty-{bot|user} - is the {bot|user}'s submission empty
        self.results = {"IDENTICAL":False, "ADEQUATE":False, "OPTIMAL":False, "EMPTY-bot":False, "EMPTY-user":False, "SCORE":0}


    def get_params(self, entity): 
        ''' A function for unpacking the parameters from default boards to be passed to the respective agent.
        
        :param entity: str; BOT or USER
        :return params: dict
        '''

        unique_nodes = set(node for pair in self.boards[entity] for node in pair[:2])
        pairs = combinations(unique_nodes, 2) # Generate all possible pairs using combinations

        # Create dictionary with keys like "LB", "LE", etc., and lookup values in entity data
        params = {
            f"{x}{y}": next(
                (v for (p1, p2, v) in self.boards[entity] if {p1, p2} == {x, y}), None
            )
            for x, y in pairs
        }
        return params
    
    def get_score(self):
        '''The function for calculating the score of the (identical) path the players agreed upon by using their given boards.'''

        score = 0
        path = self.solutions["BOT"] # the solution is identical 
        pairs = [(path[i], path[i+1]) for i in range(len(path)-1)]

        sum_board = [(a, b, el1 + el2) for [a, b, el1] in self.boards["BOT"] for [a2, b2, el2] in self.boards["USER"] if a == a2 and b == b2]

        for pair in pairs:
            score += [value[2] for value in sum_board if (pair[0] == value[0] and pair[1] == value[1]) or (pair[0] == value[1] and pair[1] == value[0])][0]

        self.results["SCORE"] = score
    
    def solution_verification(self):
        '''A function for evaluating the models' performance - checks if the generated solutions are:
            - identical
            - adequate
            - optimal
            - empty
        '''
        # if there is an empty list in the values, determine which one
        if len(self.solutions["BOT"]) == 0:
            self.results["EMPTY-bot"] = True
        if len(self.solutions["USER"]) == 0:
            self.results["EMPTY-user"] = True

         # if both players submit a solution (i.e. no empty list in the values)
        if all(self.solutions[key] for key in self.solutions):
            if self.solutions["BOT"] == self.solutions["USER"]:  # if both key dictionaries are the same
                self.results["IDENTICAL"] = True # set identical to true
                self.get_score()
                if len(self.solutions['BOT']) == (self.N+1):
                    self.results['ADEQUATE'] = True

                if self.results["SCORE"] == self.solutions["OPTIMAL"][-1]:
                    # if the calculated score is the same as the best achievable path
                    self.results["OPTIMAL"] = True

    def log(self, MODE="dialogue", MSG = "Default", ROLE = "BOT", SUMMARY = None):
        ''' Logging function;

        :param  MODE: str; default: dialogue; distinguishes the kind of 
        :param MSG: str; the message being logged, only needed for "dialogue" MODE
        :param ROLE: str; the message sender; options: BOT, USER, GSM
        :param SUMMARY: list of len 6; experiment instance metadata - 
            [
                prompt type, 
                start time / date, 
                model, 
                seed, 
                board dict (keys: "BOT", "USER"),
                optimal path(s)
            ]
        '''
        if MODE == "dialogue":
            if SUMMARY == None:
                with open(self.logging_dialogues, "a") as log_file:
                    log_file.write(f"[{ROLE}]:\n\t\t\t{MSG}\n")
                print(f"{ROLE}:\n\t{MSG}\n")

            else:
                print(f'''
                ------------------------------------------------
                                BATCH START {self.id}
                ------------------------------------------------
                Prompt type:\t{SUMMARY[0]}
                Exp start:\t{SUMMARY[1]}
                Model:\t{SUMMARY[2]}
                Seed:\t{SUMMARY[3]}
                Board ID:\t{self.BOARD_ID}
                Boards: 
                    \tBOT:\t{SUMMARY[4]["BOT"]}
                    \tUSER:\t{SUMMARY[4]["USER"]}
                    \tOPTIMAL:\t{SUMMARY[5]["OPTIMAL"]}
                ------------------------------------------------\n''')

                with open(self.logging_dialogues, "a") as log_file:
                    log_file.write(f'''
                ------------------------------------------------                   
                                BATCH START {self.id}
                ------------------------------------------------
                Prompt type:\t{SUMMARY[0]}
                Exp start:\t{SUMMARY[1]}
                Model:\t{SUMMARY[2]}
                Seed:\t{SUMMARY[3]}
                Board ID:\t{self.BOARD_ID}
                Boards: \t\tBOT:\t{SUMMARY[4]["BOT"]}
                        \t\tUSER:\t{SUMMARY[4]["USER"]}
                        \t\tOPTIMAL:\t{SUMMARY[5]["OPTIMAL"]}
                ------------------------------------------------\n''')
                    
                with open(self.logging_GSM, "a") as log_file:
                    log_file.write(f'''
                ------------------------------------------------                   
                                BATCH START {self.id}
                ------------------------------------------------
                Prompt type:\t{SUMMARY[0]}
                Exp start:\t{SUMMARY[1]}
                Model:\t{SUMMARY[2]}
                Seed:\t{SUMMARY[3]}
                Board ID:\t{self.BOARD_ID}
                Boards: \t\tBOT:\t{SUMMARY[4]["BOT"]}
                        \t\tUSER:\t{SUMMARY[4]["USER"]}
                        \t\tOPTIMAL:\t{SUMMARY[5]["OPTIMAL"]}
                ------------------------------------------------\n''')
                    
        elif MODE == "eval":
            with open(self.logging_eval, "a") as log_file:
                log_file.write(f'''
        ------------------------------------------------
                        EVALUATION BREAKDOWN {self.id}
        ------------------------------------------------
                               BASICS
        ------------------------------------------------                       
        Experiment start:\t{self.run_start}
        Prompt type:\t{self.prompt_type}
        Model:\t{self.model}
        Seed:\t{self.seed}
        Board ID:\t{self.BOARD_ID}
        Starting boards:
                \t\tBOT:\t{self.boards["BOT"]}
                \t\tUSER:\t{self.boards["USER"]}
        ------------------------------------------------
                        FINAL PATHS
        ------------------------------------------------
        BOT:\t{self.solutions["BOT"]}
        USER:\t{self.solutions["USER"]}
        OPTIMAL:\t{self.solutions["OPTIMAL"]}
        ------------------------------------------------
                        EVALUATION RESULTS
        ------------------------------------------------
        \tScore:\t{self.results["SCORE"]}
        \tIdentical:\t{self.results["IDENTICAL"]}
        \tOptimal:\t{self.results["OPTIMAL"]}
        \tAdequate:\t{self.results["ADEQUATE"]}
        \tEmpty-bot:\t{self.results["EMPTY-bot"]}
        \tEmpty-user:\t{self.results["EMPTY-user"]}
        ------------------------------------------------
''')
        elif MODE == "eval-terminated":
            with open(self.logging_eval, "a") as log_file:
                log_file.write(f'''
        ------------------------------------------------
                        EVALUATION BREAKDOWN {self.id}
        ------------------------------------------------
                               BASICS
        ------------------------------------------------                       
        Experiment start:\t{self.run_start}
        Prompt type:\t{self.prompt_type}
        Model:\t{self.model}
        Seed:\t{self.seed}
        Board ID:\t{self.BOARD_ID}
        Starting boards:
                \t\tBOT:\t{self.boards["BOT"]}
                \t\tUSER:\t{self.boards["USER"]}
        ------------------------------------------------
                    FINAL PATHS -- TERMINATED
        ------------------------------------------------
        BOT:\t{self.solutions["BOT"]}
        USER:\t{self.solutions["USER"]}
        OPTIMAL:\t{self.solutions["OPTIMAL"]}
        ------------------------------------------------''')
    
    def parse_message(self, MESSAGE):

        ''' A function that uses regex to parse the given MESSAGE and extract flags (Thought, Action, Message).

        :param MESSAGE: str
        :return result: dict (key - flag (str); val - contents (str)); the extracted data
        '''
        result = {}

        # regex to match the pattern [FLAG]: followed by content
        # capture everything after a flag until the next flag or the end of the string
        try:
            pattern = r'\[([^\]]+)\]:(.*?)(?=\[\w+\]|$)'
            matches = re.findall(pattern, MESSAGE, flags=re.DOTALL)

            if not matches:  # if matches is empty, raise an exception to trigger the except block (format issue, no doppelpunkt)
                raise Exception("No matches found with initial pattern")

            for flag, content in matches:
                result[flag.strip()] = content.strip()

        except:
            pattern = r'\[([^\]]+)\](.*?)(?=\[\w+\]|$)'
            matches = re.findall(pattern, MESSAGE, flags=re.DOTALL)

            for flag, content in matches:
                result[flag.strip()] = content.strip()

        return result

    def parse_actions(self,actions, ROLE="BOT"):
        
        pattern = r'end\((.*?)\)'
        matches = re.findall(pattern, actions)
        if matches:
            FINAL_PATH = [ast.literal_eval(match) for match in matches][0]
            self.solutions[ROLE] = FINAL_PATH
            self.end[ROLE] = True
        else:
            self.actions_history[ROLE].append(actions)

    def check_end(self):

        for key in self.actions_history.keys():
            for el in self.actions_history[key]:
                if el.startswith("end("):
                    self.end[key] = True

    def run(self, start, TURNS):
        # logging the basics into the dialogue file 
        self.log(SUMMARY=[self.prompt_type, self.run_start, self.model, self.seed, self.boards, self.solutions])
        self.user_proxy.update_message_history(start)

        while self.playing:
            # terminate the program in case the agents get stuck in a loop
            TURNS -= 1
            if TURNS == 0:
                break

            # adjusting the message containers:

            self.check_end()

            if self.end["USER"] and self.end["BOT"]:
                self.solution_verification()
                self.playing = False
                break

            user_message = self.user_proxy.get_inference() # messages generated by the user proxy:
            self.log(MSG=user_message, ROLE="USER") # log user message
            user_message_PARSED = self.parse_message(user_message)
            
            try:
                new_string = f"[World-state-own] {self.boards["BOT"]}\n[History] {self.actions_history["BOT"]}\n[Observation] {user_message_PARSED["Message"]}" # format input for bot
            except:
                new_string = f"[World-state-own] {self.boards["BOT"]}\n[History] {self.actions_history["BOT"]}\n[Observation] <USER MESSAGE IN WRONG FORMAT>" # format input for bot
            self.bot.update_message_history(new_string)
            self.log(MSG=new_string, ROLE="BOT-input")

            # update corresponding actions_history list
            try:
                self.parse_actions(user_message_PARSED["Action"], "USER")
            except:
                pass

            if self.end["USER"] and self.end["BOT"]:
                self.solution_verification()
                self.playing = False
                break

            else:
                bot_message = self.bot.get_inference()
                self.log(MSG=bot_message, ROLE="BOT") # log bot message
                bot_message_PARSED = self.parse_message(bot_message) # extract bot actions
                try:
                    self.parse_actions(bot_message_PARSED["Action"], "BOT")
                except:
                    pass

            
                try:

                    new_string = f"[World-state-own] {self.boards["USER"]}\n[History] {self.actions_history["USER"]}\n[Observation] {bot_message_PARSED["Message"]}" # format input for bot
                except:
                    new_string = f"[World-state-own] {self.boards["USER"]}\n[History] {self.actions_history["USER"]}\n[Observation] <USER MESSAGE IN WRONG FORMAT>" # format input for bot
                self.user_proxy.update_message_history(new_string)
                self.log(MSG=new_string, ROLE="USER-input")

                if self.end["USER"] and self.end["BOT"]:
                    self.solution_verification()
                    self.playing = False
                    break

        # Log the eval results to a separate eval file
        if not (self.end["USER"] and self.end["BOT"]):
            self.log(MODE='eval-terminated')
        else:
            self.log(MODE="eval")

class BotInstance:
     
    def __init__(self, BASE_PROMPT:str, MODEL:str, SEED:int, MAXTOKENS:int, BOARD:list):
        ''' 
        A class for the agents playing the game;

        :param BASE_PROMPT: str; the starting system prompt describing the task + example
        :param MODEL: str; OpenAI model
        :param SEED: int;
        :param MAXTOKENS: int;
        :param BOARD: list; the agent's graph. format [("edge1", "edge2", intval), ...]
        '''
        self.msg_history = [{"role":"system", "content":BASE_PROMPT}]
        self.model = MODEL # gpt-4o-2024-08-06
        self.seed = SEED
        self.MAXTOKENS = MAXTOKENS
        self.BOARD = BOARD # own board

    def inference(self): 
        ''' A function that calls the OpenAI API using the previously set parameters

        :return output: str; model output
        '''
        client = OpenAI(api_key=OPENAI_API_KEY)
        completion = client.chat.completions.create(
        model=self.model,
        messages=self.msg_history,
        max_tokens=self.MAXTOKENS,
        temperature=1,
        seed=self.seed
        )
        output = completion.choices[0].message.content        
        return output

    def get_inference(self):

        raw_output = self.inference()
        self.msg_history.append({"role":"assistant", "content":raw_output})
        self.recent_message = raw_output

        return raw_output
    
    def update_message_history(self,new_message):
        self.msg_history.append({"role":"user", "content":new_message})
                
def main():

    NODES = 6
    BOARDS_PATH = f"boards_{NODES}.json"
    prompt_type = ""
    n = 5
    version = "full-v3-new-actions-redo"
    setup = f"{NODES}nodes"

    for seed in [1011, 143, 9999, 8060]:
        run_start = datetime.now()
        path = f"logs/{version}/{setup}"
        if not os.path.exists(path):
            os.makedirs(path)
        
        logging_didalogues = f"{path}/{seed}-{run_start}-dialogues.txt"
        logging_eval = f"{path}/{seed}-{run_start}-eval.txt"
        logging_GSM = f"{path}/{seed}-{run_start}-GSM.txt"

        with open(BOARDS_PATH, "r") as file:
            boards = json.load(file)
        
        for i in range(n):
        # send a random board from json file
            ID = f"{seed}-{i+20}"
            # board_index = str(randint(1,6))
            board_index = str(choices(list(range(1,7)), [0.1, 0.1, 0.2, 0.2, 0.2, 0.2])[0])
            board = boards[board_index]
            game = GAME(NODES, board, seed, logging_didalogues, logging_eval, logging_GSM, board_index, ID) # start a game instance
            start = f"[World-state-own] {game.boards["USER"]}\n[History] {game.actions_history["USER"]}\n[Observation] <START>"
            game.run(start, TURNS=16)


if __name__ == "__main__":
    main()